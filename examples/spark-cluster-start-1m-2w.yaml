apiVersion: spark.stackable.de/v1
kind: SparkCluster
metadata:
  name: spark-cluster
spec:
  master:
    selectors:
      - selector: 
         matchLabels:
           kubernetes.io/hostname: "bawa-virtualbox"
    tolerations:
      - key: "kubernetes.io/arch"
        operator: "Equal"
        effect: "NoSchedule"
        value: "stackable-linux"
      - key: "kubernetes.io/arch"
        operator: "Equal"
        effect: "NoExecute"
        value: "stackable-linux"
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "node.kubernetes.io/not-ready"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 300
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 300
    instances: 1
    # no cpu defaults to take all
    cores: "1"
    # no memory defaults to take all
    memory: "2g"
    commands:
      - "spark-3.0.1-bin-hadoop2.7/sbin/start-master.sh"
    # spark-defaults.conf
    sparkConfiguration:
    # required for secret
    - name: "spark.authenticate"
      value: "true"
    # actual secret for secret
    - name: "spark.authenticate.secret"
      value: "stackable_secret" 
    # spark-env.sh
    env:
#      - name: SPARK_HOME
#        value: "{{packageroot}}/spark-3.0.1-bin-hadoop2.7"
      - name: "SPARK_CONF_DIR"
        value: "{{configroot}}/conf"
#      - name: "SPARK_MASTER_PORT"
#        value: "7077"
        # do not fork processes
      - name: "SPARK_NO_DAEMONIZE"
        value: "true"
  worker:
    selectors:
      - selector: 
         matchLabels:
           kubernetes.io/hostname: "bawa-virtualbox"
    tolerations:
      - key: "kubernetes.io/arch"
        operator: "Equal"
        effect: "NoSchedule"
        value: "stackable-linux"
      - key: "kubernetes.io/arch"
        operator: "Equal"
        effect: "NoExecute"
        value: "stackable-linux"
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "node.kubernetes.io/not-ready"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 300
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 300
    # number of workers
    instances: 2 
    # no cpu defaults to take all
    cores: "1"
    # no memory defaults to take all
    memory: "4g"
    # end of customization
    commands:
      # hostname and port provided from operator: start-slave.sh args: spark://MASTER_URL:MASTER_PORT
      - "spark-3.0.1-bin-hadoop2.7/sbin/start-slave.sh"
    # spark-defaults.conf
    sparkConfiguration:
    - name: "spark.authenticate"
      value: "true"
    - name: "spark.authenticate.secret"
      value: "stackable_secret" 
    # spark-env.sh
    env:
#      - name: SPARK_HOME
#        value: "{{packageroot}}/spark-3.0.1-bin-hadoop2.7"
      - name: "SPARK_CONF_DIR"
        value: "{{configroot}}/conf"
#      - name: "SPARK_MASTER_PORT"
#        value: "7077"
        # do not fork processes
      - name: "SPARK_NO_DAEMONIZE"
        value: "true"
  image: "spark:3.0.1"
  metrics: false
